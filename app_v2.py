import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, AIMessage
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
@st.cache_resource
def init_components():
    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
    llm = ChatOpenAI(
        model=st.secrets.openrouter.model,
        openai_api_key=st.secrets.openrouter.api_key,
        openai_api_base=st.secrets.openrouter.api_base,
        temperature=0
    )

    # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–µ—Ä–∞
    embedding = HuggingFaceEmbeddings(model_name="ai-forever/FRIDA")

    # 3. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ ChromaDB
    vectordb = Chroma(
        persist_directory=st.secrets.chroma.chroma_path,
        collection_name=st.secrets.chroma.collection_name,
        embedding_function=embedding
    )

    # 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
    retriever = vectordb.as_retriever(
        search_kwargs={"k": 3,},
        search_type="similarity"
    )

    # 5. –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    template = """
    You are an assistant for working with Sinamics S120. Answer the questions only in Russian, using the provided context.
    If you don't have the necessary information in the context, say you don't know the answer.

    Context: {context}

    Question: {question}

    Detailed and complete answer:"""

    qa_prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )

    # 6. –°–æ–∑–¥–∞–Ω–∏–µ RAG —Ü–µ–ø–∏
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": qa_prompt},
        return_source_documents=True
    )

    return llm, qa_chain

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
llm, qa_chain = init_components()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —á–∞—Ç–∞
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.use_rag = True
    st.session_state.show_sources = True

# –°–∞–π–¥–±–∞—Ä —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
with st.sidebar:
    st.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
    st.session_state.use_rag = st.checkbox(
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π",
        value=st.session_state.use_rag
    )
    if st.session_state.use_rag:
        st.session_state.show_sources = st.checkbox(
            "–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏",
            value=st.session_state.show_sources
        )

def format_sources(docs):
    sources = []
    for i, doc in enumerate(docs):
        source_name = doc.metadata.get("source", "")
        sources.append(
            f"##### üìÑ –ò—Å—Ç–æ—á–Ω–∏–∫ {i+1}: {source_name}\n"
            f"{doc.page_content}\n"
            f"---"
        )
    return "\n\n".join(sources)

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
for message in st.session_state.messages:
    role = "user" if isinstance(message, HumanMessage) else "assistant"
    with st.chat_message(role):
        st.write(message.content)

        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –∏ –≤–∫–ª—é—á–µ–Ω–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
        if (role == "assistant" and
            "source_docs" in message.additional_kwargs and
            st.session_state.show_sources):
            with st.expander("üîç –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏"):
                st.markdown(format_sources(message.additional_kwargs["source_docs"]))

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
if prompt := st.chat_input("–í–∞—à –≤–æ–ø—Ä–æ—Å –æ Sinamics S120..."):
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_message = HumanMessage(content=prompt)
    st.session_state.messages.append(user_message)

    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    with st.chat_message("user"):
        st.write(prompt)

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    with st.chat_message("assistant"):
        with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é..."):
            try:
                if st.session_state.use_rag:
                    # –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π RAG –∑–∞–ø—Ä–æ—Å
                    result = qa_chain({"query": prompt})

                    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–ª–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
                    response = AIMessage(
                        content=result["result"],
                        additional_kwargs={
                            "source_docs": result["source_documents"]
                        }
                    )
                else:
                    # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
                    response = llm.invoke(st.session_state.messages)

                # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –æ—Ç–≤–µ—Ç
                st.write(response.content)

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
                if (st.session_state.use_rag and
                    st.session_state.show_sources and
                    "source_docs" in response.additional_kwargs):

                    with st.expander("üîç –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏"):
                        st.markdown(format_sources(response.additional_kwargs["source_docs"]))

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
                st.session_state.messages.append(response)

            except Exception as e:
                st.error("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞")
                error_message = AIMessage(content="‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ")
                st.session_state.messages.append(error_message)
